{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import dataset (1 mark)\n",
    "total_data = pd.read_csv('nba_2022_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?  \n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?  \n",
    " \n",
    " *ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddb6d2-5067-43f6-bfd4-21d1dad0c1ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.  Bryan Weather Chung, October 12/2023, \"NBA Player Stats Dataset for the 2022-2023\", Kaggle. [Online].    Available : https://www.kaggle.com/datasets/bryanchungweather/nba-players-data-2022-2023/data    \n",
    "  \n",
    "  \n",
    "2.   I'm a big fan of the NBA, and I thought it would be cool to make a machine learning model on a dataset with NBA players statistics. My goal is to make a target vector of points per game, using the rest of the columns as features. I may drop some features such as \"Player\" because we don't need to know a players name predict a players points, just the players statistics.  \n",
    "  \n",
    "  \n",
    "3.  Nope, I just went to kaggle found the most trending datasets and found this was one of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06f7c92e-5232-4a44-81e4-0a26526b7219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3P</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>2P</th>\n",
       "      <th>2PA</th>\n",
       "      <th>2P%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>FT</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precious Achiuwa</td>\n",
       "      <td>C</td>\n",
       "      <td>23</td>\n",
       "      <td>TOR</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>20.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.269</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.521</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.702</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steven Adams</td>\n",
       "      <td>C</td>\n",
       "      <td>29</td>\n",
       "      <td>MEM</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.597</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.364</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bam Adebayo</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>MIA</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>34.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.083</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.541</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.806</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ochai Agbaji</td>\n",
       "      <td>SG</td>\n",
       "      <td>22</td>\n",
       "      <td>UTA</td>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.427</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Santi Aldama</td>\n",
       "      <td>PF</td>\n",
       "      <td>22</td>\n",
       "      <td>MEM</td>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "      <td>21.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.353</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player Pos  Age   Tm   G  GS    MP   FG   FGA    FG%   3P  3PA  \\\n",
       "0  Precious Achiuwa   C   23  TOR  55  12  20.7  3.6   7.3  0.485  0.5  2.0   \n",
       "1      Steven Adams   C   29  MEM  42  42  27.0  3.7   6.3  0.597  0.0  0.0   \n",
       "2       Bam Adebayo   C   25  MIA  75  75  34.6  8.0  14.9  0.540  0.0  0.2   \n",
       "3      Ochai Agbaji  SG   22  UTA  59  22  20.5  2.8   6.5  0.427  1.4  3.9   \n",
       "4      Santi Aldama  PF   22  MEM  77  20  21.8  3.2   6.8  0.470  1.2  3.5   \n",
       "\n",
       "     3P%   2P   2PA    2P%   eFG%   FT  FTA    FT%  ORB  DRB   TRB  AST  STL  \\\n",
       "0  0.269  3.0   5.4  0.564  0.521  1.6  2.3  0.702  1.8  4.1   6.0  0.9  0.6   \n",
       "1  0.000  3.7   6.2  0.599  0.597  1.1  3.1  0.364  5.1  6.5  11.5  2.3  0.9   \n",
       "2  0.083  8.0  14.7  0.545  0.541  4.3  5.4  0.806  2.5  6.7   9.2  3.2  1.2   \n",
       "3  0.355  1.4   2.7  0.532  0.532  0.9  1.2  0.812  0.7  1.3   2.1  1.1  0.3   \n",
       "4  0.353  2.0   3.4  0.591  0.560  1.4  1.9  0.750  1.1  3.7   4.8  1.3  0.6   \n",
       "\n",
       "   BLK  TOV   PF   PTS  \n",
       "0  0.5  1.1  1.9   9.2  \n",
       "1  1.1  1.9  2.3   8.6  \n",
       "2  0.8  2.5  2.8  20.4  \n",
       "3  0.3  0.7  1.7   7.9  \n",
       "4  0.6  0.8  1.9   9.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our goal is to make a machine learning model that will predict a NBA players points per game. Let us check if \n",
    "# we have any columns that would be irrelevant to determing a players points per game.\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8345a1f-5ff2-4018-82de-a0670268fb52",
   "metadata": {
    "tags": []
   },
   "source": [
    "All these columns are relevant for determing a players points per game. For example, the \"Player\" column names a \n",
    "player. We could use a players past results to help determine a players future results. The \"Tm\" column names a \n",
    "team. This is very important because many teams have different coaches and philosphies on how basketball should\n",
    "be played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afc244d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player     0\n",
       "Pos        0\n",
       "Age        0\n",
       "Tm         0\n",
       "G          0\n",
       "GS         0\n",
       "MP         0\n",
       "FG         0\n",
       "FGA        0\n",
       "FG%        3\n",
       "3P         0\n",
       "3PA        0\n",
       "3P%       24\n",
       "2P         0\n",
       "2PA        0\n",
       "2P%        7\n",
       "eFG%       3\n",
       "FT         0\n",
       "FTA        0\n",
       "FT%       37\n",
       "ORB        0\n",
       "DRB        0\n",
       "TRB        0\n",
       "AST        0\n",
       "STL        0\n",
       "BLK        0\n",
       "TOV        0\n",
       "PF         0\n",
       "PTS        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "# Check for any null values\n",
    "null_values = total_data.isnull().sum()\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7318a4ab-e921-4b8a-8a23-e55e8ae4e1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace null values with mean of the columns\n",
    "total_data = total_data.fillna(total_data.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8a4a722-ac20-4c66-b258-75aed1e0f675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player    0\n",
       "Pos       0\n",
       "Age       0\n",
       "Tm        0\n",
       "G         0\n",
       "GS        0\n",
       "MP        0\n",
       "FG        0\n",
       "FGA       0\n",
       "FG%       0\n",
       "3P        0\n",
       "3PA       0\n",
       "3P%       0\n",
       "2P        0\n",
       "2PA       0\n",
       "2P%       0\n",
       "eFG%      0\n",
       "FT        0\n",
       "FTA       0\n",
       "FT%       0\n",
       "ORB       0\n",
       "DRB       0\n",
       "TRB       0\n",
       "AST       0\n",
       "STL       0\n",
       "BLK       0\n",
       "TOV       0\n",
       "PF        0\n",
       "PTS       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See if any null values remain\n",
    "null_values = total_data.isnull().sum()\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70a8c127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "\n",
    "# We will use columns transformer to apply One-Hot-Encoding to columns with categorical variables.These columns are \n",
    "# \"Player\", \"Pos\" and \"Tm\" refering to a players name, their position, and their team. We will also scale the data \n",
    "# in case we use models that are senstivie to unscaled data.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "columns_to_scale = total_data.columns.difference(['Player', 'Pos', 'Tm', 'PTS'])\n",
    "columns_to_encode = ['Player', 'Pos', 'Tm']\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        (\"scaling\", StandardScaler(), columns_to_scale),\n",
    "        (\"onehot\", OneHotEncoder(sparse_output=False, handle_unknown ='ignore'), columns_to_encode)\n",
    "        ])\n",
    "\n",
    "X = total_data.drop(\"PTS\", axis=1)\n",
    "y = total_data[\"PTS\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ced0a-7016-4ded-a1b8-d925cdd8574a",
   "metadata": {},
   "source": [
    "1. Yes, a few null values appeared in the dataset. I replaced them by the mean of the columns. Since the missing data was all from numerical columns, I could easily fill them in with the mean of the their respective columns. Also, the missing data was all statistics of a player, it would make much more sense to fill in the missing data with the mean of the column instead of backfilling or front filling. This is because all the rows are unrelated, but the column is related.  \n",
    "  \n",
    "  \n",
    "2. My data is mostly numerical data describing the statistics of a nba player in the 2022 - 2023 season. Their are some categorical values such as \"Player\", \"Pos\", and \"Tm\". These three columns describe a players name, their position, and their team. The preprocessing methods I used are One-Hot-Encoding for columns with categorical values I just decribed. I also scaled the data in case I choose to use a model that is sensitive to unscaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e8e7a65-3db3-4641-a30d-fb62eab8f87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Note: I choose a small amount of parameters since my laptop was taking a while to execute a lot of parameters\n",
      "\n",
      "Best Model Parameters for RandomForest, SVM, and Lasso\n",
      "\n",
      "Random Forest Results:\n",
      "Best Parameters: {'model__max_depth': 15, 'model__n_estimators': 10}\n",
      "Grid Search Best Results: 0.9872112796119143\n",
      "\n",
      "SVM Results:\n",
      "Best Parameters: {'model__C': 0.1, 'model__gamma': 0.1, 'model__kernel': 'poly'}\n",
      "Grid Search Best Results: 0.8007287887722742\n",
      "\n",
      "Lasso Results:\n",
      "Best Parameters: {'model__alpha': 0.1, 'model__max_iter': 1000}\n",
      "Grid Search Best Results: 0.9994697072325331\n"
     ]
    }
   ],
   "source": [
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "\n",
    "# We will use two non-linear models, RandomForest and SVM. We will use one linear model ridge regression.First, we\n",
    "# create pipeline for all 3 models, select grid paramaters, than apply GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random Forest Pipeline and parameter grid for Random Forest\n",
    "rf_pipeline = Pipeline([ ('preprocessor', ct), ('model', RandomForestRegressor()) ])\n",
    "\n",
    "rf_param_grid = { \n",
    "        'model__n_estimators' : [5, 10, 15], \n",
    "        'model__max_depth': [5, 10, 15]\n",
    "        }\n",
    "\n",
    "#SVM Pipeline and parameter grid for SVM\n",
    "svm_pipeline = Pipeline([ ('preprocessor', ct), ('model', SVR()) ])\n",
    "\n",
    "svm_param_grid = {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__kernel': ['poly', 'rbf'],\n",
    "        'model__gamma': [0.1, 1, 10]\n",
    "        }\n",
    "\n",
    "# Ridge regression pipeline and parameter grid for ridge\n",
    "Lasso_pipeline = Pipeline([ ('preprocessor', ct), ('model', Lasso()) ])\n",
    "\n",
    "Lasso_param_grid = {\n",
    "        'model__alpha': [0.1, 1, 10],\n",
    "        'model__max_iter': [1000, 5000, 10000]\n",
    "        }\n",
    "\n",
    "# Apply GridSearchCV to all models and their respective parameter grids\n",
    "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv = 5, n_jobs = -1)\n",
    "svm_grid_search = GridSearchCV(svm_pipeline, svm_param_grid, cv = 5, n_jobs = -1)\n",
    "Lasso_grid_search = GridSearchCV(Lasso_pipeline, Lasso_param_grid, cv = 5, n_jobs = -1)\n",
    "\n",
    "# Fit the models\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "Lasso_grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Results for all three models - mean_test_score\n",
    "rf_results = rf_grid_search.cv_results_['mean_test_score']\n",
    "svm_results = svm_grid_search.cv_results_['mean_test_score']\n",
    "Lasso_results = Lasso_grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Print Best Parameters and Results\n",
    "print(\"Please Note: I choose a small amount of parameters since my laptop was taking a while to execute a lot of parameters\")\n",
    "print(\"\\nBest Model Parameters for RandomForest, SVM, and Lasso\\n\")\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"Best Parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Grid Search Best Results:\", rf_results[rf_grid_search.best_index_])\n",
    "\n",
    "print(\"\\nSVM Results:\")\n",
    "print(\"Best Parameters:\", svm_grid_search.best_params_)\n",
    "print(\"Grid Search Best Results:\", svm_results[svm_grid_search.best_index_])\n",
    "\n",
    "print(\"\\nLasso Results:\")\n",
    "print(\"Best Parameters:\", Lasso_grid_search.best_params_)\n",
    "print(\"Grid Search Best Results:\", Lasso_results[Lasso_grid_search.best_index_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aeb3da-db55-4868-9780-3728b36f284e",
   "metadata": {},
   "source": [
    "1. Regression, my targets vector is 'PTS' which represents points a player gets per game, these are all numerical values.  \n",
    "  \n",
    "  \n",
    "2. I used Random Forest, SVM, and ridge models. The reason I used Random Forest was because from my past experiences, it appears to be one of the most powerful models and relatively quick as well. Since my One-Hot-Encoding made this dataset very high deminesional, I thought this would be a good dataset to see the limitations of Random Forest. I also chose this model because gradient boosting machines requires an iterative approach, and that would take much longer to execute.      \n",
    "\n",
    "    I used a SVM model as well. SVM models work well with high dimensional data, so I thought it would be a good fit for this dataset. Since the number of features exploded with One-Hot-Encoding. Another reason I this model was because I wanted to get a better understanding on how the pipeline class works, and since SVM model requires scaling, it was a good choice to improve my understanding of the pipeline. I also wanted to see how changing C and gamma would impact the results.  \n",
    "    \n",
    "    For my linear model, I choose lasso regression. The reason I choose lasso was because I knew that many features would be irrelevant when training my model. So it was important, that we could apply L1 regularization and put some coefficants for some features to exactly 0.  \n",
    "      \n",
    "3. The best model was the lasso model. It had a R2 score of 0.9995. Much better than Random Forest which had a R2 score of 0.98721 and SVM which had a R2 score of 0.80073. This does make sense since linear models typically do well when the number of features is large compared to number of samples.   \n",
    "  \n",
    "    Random Forest might have done worse than my Lasso model because the dataset might not have been large enough. We know Random Forest does better with large datasets but this dataset has only 680 samples. Also Random Forest considers all features, where as Lasso only considers the relevant features. \n",
    "    \n",
    "    Same thing can be said about SVM, where SVM considers all features while Lasso only considers the most important features. We also might have chose the wrong type of scaling for this dataset, maybe MinMaxScalar would have been better for this dataset.  \n",
    "      \n",
    "    It is important to note for this dataset, we have a linear relationships between many features and the target vector. For example, feature \"FG%\" and our target vector \"PTS\" share a linear relationship. Since our lasso model works best with linear models, this is probably the biggest reason Lasso model did the best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69e64c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995043866770325\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "# The best model was the Lasso Model\n",
    "\n",
    "lasso_result = Lasso_grid_search.score(X_test, y_test)\n",
    "print(lasso_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046daae-e90d-4a21-a336-fc90273afd9a",
   "metadata": {},
   "source": [
    "1. I used the R2 accuracy metric.  \n",
    "  \n",
    "2.  The results are remarkably similar. We got a result of 0.9995 on the best result from GridSearchCV. Here, for our validation test, we got a R2 score of 0.9995 as well. In fact, it's a bit higher if we don't round it to the 4th decimal place. Yes the model generalized extremely well.\n",
    "  \n",
    "3. Yes, the model peformed exceptionally, and it is certainly able to be used in the real world. A R2 score of 0.9995 is nearly perfect and I am sure it would work well in the real world if released. This model also does not evaluate anything critical such as health of a patient or future floods. It is a simple model that simply evaluates how much pts per game a NBA player would have in a season based on some statistics. It will have no major harmful impact on the real world. I do think we can improve the results using ElasticNet model. This will allow us to use L1 and L2 regularization to improve our scores. I could also check a wider range for alpha instead of 0.1 to 10. Unfortunately, It does take a lot of time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2549857-e5cd-4f23-85e0-5b7b9190b3f7",
   "metadata": {},
   "source": [
    "1. Bryan Weather Chung, October 12/2023, \"NBA Player Stats Dataset for the 2022-2023\", Kaggle. [Online].    Available : https://www.kaggle.com/datasets/bryanchungweather/nba-players-data-2022-2023/data  \n",
    "  \n",
    "2. First I found the dataset that I was interested in. I read in the data using pandas. I started data processing, by checking all columns for categorical values. I filled in any missing values with the mean of that respective column. I loaded scaling and One-Hot-Encoding into the columnTransformer class for our pipeline class to implement. Afterwords, I split the data into my target vector and feature matrix, and used test_train_split to split the data into testing and training portions. I than created 3 pipeline classes with 3 different models. I then used GridSearchCV to create find the best parameters of each model. Once the best model was found with the best training score, we used the testing set to check our validation score. Then, we finally reported our validation score.  \n",
    "  \n",
    "3. Yes I used chatGPT for this assignment. I prompted chatGPT to for what was stored in \"cv_results_\" of \"GridSearchCV\". I was having trouble finding the testing scores for the models and asked chatGPT to explain \"cv_results_\" and what was stored in that attribute. I also asked chatGPT how to make \"GridSearchCV\" faster as well because it was taking so long. So I added \"n_jobs = -1\" to use all available CPU's. I had to modify the code to add \"n_jobs\" and \"mean_test_score\" to find the results of GridSearchCV execution.  \n",
    "  \n",
    "4. Yes, I struggled with GridSearchCV class. I had trouble with how long it was taking to execute. I also had trouble understanding the cv_results_ attribute of GridSearchCV. Their were many variables in that attribute and I struggled understanding what each one meant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2289ec-813b-4a46-bd21-aab49de83e55",
   "metadata": {},
   "source": [
    "I liked learning about pipelines and GridSearchCV. Very powerful tools that allow us to find the best models with the best parameters. I disliked how long it took to execute GridSearchCV. What I found interesting and confusing was the cv_results_ attribute of GridSearchCV. It holds a lot of different information that can be used in many differeny ways. I also found using columntransformers as a preprocessing step in pipelines is a very powerful and convenient way to preprocess data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
